\mychapter{3}{Lesson 3} %181003

\section{Randomness Extraction}
Randomness extraction is the process of extracting real randomness from imperfect randomness.


\subsection{Von Neumann extractor}
Take a biased coin $B$ such that $\Pr[B = 0] = p \neq \sfrac{1}{2}$ and $\Pr[B = 1] = 1 - p$.
The Von Neumann extractor is the algorithm
%
\begin{algorithm}[H]
	\caption{Von Neumann extractor}\label{alg:von-neumann-extractor}
	\begin{algorithmic}
		\Do
			\State sample $b_1 \gets B$
			\State sample $b_2 \gets B$
		\doWhile{$b_1 \neq b_2$}
		\Return{$b_2$}
	\end{algorithmic}
\end{algorithm}

$b_1$ and $b_2$ are \iid, thus the probability of both outputs is $\Pr[b_1 = 0 \wedge b_2 = 1] = \Pr[b_1 = 1 \wedge b_2 = 0] = p(p-1)$.
The probability of outputting something in a given iteration is $2p(p-1)$, therefore the probability of failing after $n$ iterations is $\Pr[\text{fail }n \text{ times}] = [1 -2p(1-p)]^n$, which is very small (it follows the geometric distribution).


\subsection{Unseeded extraction}
In real life there are sources of unpredictability and their randomness is measured through min-entropy.
The min-entropy depends on the probability of the most frequent event, a sort of worst-case scenario for randomness.

\begin{definition}[Min-entropy]
	The min-entropy $H_{\infty}$ of a random variable $X$ is defined as $H_{\infty}(X) = - \log_2 \max \Pr[X = x]$.
\end{definition}

\begin{example}
	Take $X = \unif{\Bool^n}$, then $H_{\infty} = - \log_2 \frac{1}{2^n} = n$.
	Now, take $X$ such that $\Pr[X = 0^n] = 1$ and $\Pr[X \neq 0^n] = 0$ (it is always $0^n$), then $H_{\infty} = - \log_2 1 = 0$.
\end{example}

\begin{definition}
	An $(n,k)$-source is a random variable $X \in \Bool^n$ such that  $H_{\infty}(X) \geq k$.
\end{definition}

We want to create a function  $\Ext$ that extracts perfect randomness from a $(n,k)$-source that works for all possible $(n,k)$-sources.
However, this is not possible even if $\Ext : \Bool^n \to \Bool$ and $k = n - 1$.
\begin{proof}
	Let $\Ext : \Bool^n \to \Bool$ be any candidate extractor.
	Let $b \in \Bool$ such that $|\Ext^{-1}(b)| \geq 2^{n-1}$ (the most frequent case).
	
	\begin{figure}[H]
		\centering
		\input{tikz/3-unseeded-extractor.tikz}
		\caption{The candidate extractor $\Ext$.}
	\end{figure}
	
	
	Let the random variable $X_{\text{bad}}$ be uniform over $\Ext^{-1}(b)$.
	Then, $\Ext(X_\text{bad}) = b$ (always the same) and $H_{\infty}(X_\text{bad}) \geq n - 1$.
\end{proof}


\subsection{Seeded extraction}
\begin{definition}[Statistical distance]
	Let $X$ and $X'$ be two random variables over the set $\X$.
	Their statistical distance $\SD$ is defined as
	\[
		\SD(X, X') = \frac{1}{2}\sum_{x \in \X } \abs{\Pr[X=x] - \Pr[X'=x]} \, .
	\]
\end{definition}
%
\noindent It is $0$ when $X$ and $X'$ follow the same distribution, it is $1$ when they have no possible events in common.

\begin{definition}[Seeded extractor]
	Let $U_l \sim \unif{\Bool^l}$ and $S \sim \unif{\Bool^d}$.
	A function
	\[
		\Ext: \underbrace{\Bool^d}_{\text{seed (public})} \times \underbrace{\Bool^n}_{\text{input}} \to \underbrace{\Bool^l}_{\text{output}}
	\]
	is a $(k, \eps)$-extractor if $\forall X \in \Bool^n : H_{\infty}(X) \geq k$
	\[
		\SD((S, \Ext(S, X)), (S, U_l)) \leq \eps \, .
	\]
\end{definition}
%
\noindent Even though the seed is public, the output of the extractor cannot be distinguished from a random string.

\begin{lemma}[Leftover hash lemma]\label{lemma:leftover-hash}
	Let $\H = \{h_S : \Bool^n \to \Bool^l\}_{S \in \Bool}$ be a pairwise independent family (that includes an extractor for each seed).
	Then, $\Ext(s, x) = h_s(x)$ is a $(k, \eps)$-extractor for $k \geq l + 2 \log \frac{2}{\eps} -2$.
\end{lemma}
%
\noindent Informally, the leftover hash lemma states that the length of the output must be less than $k$.
In order to prove it, we need several other elements.

\begin{definition}[Collision probability]\label{def:col}
	Let $Y$ and $Y'$ two \iid{} random variables over a set $\Y$.
	Then, the collision probability of $Y$ is defined as
	\[
		\Col(Y) = \Pr[Y = Y'] = \sum_{y \in \Y} \Pr[Y = y]^2
	\]
	and is the probability that both variables assume the same value.
\end{definition}

\begin{exercise}\label{ex:leftover-hash-pre-1}
	Let $X$ be a random variable such that $H_{\infty}(X) \geq k$.
	Then,
	\begin{equation}\label{eq:leftover-hash-pre-1}
		\Col(X) \leq 2^{-k} \, .
	\end{equation}
\end{exercise}
\todo{Solution}

\begin{exercise}\label{ex:leftover-hash-pre-2}
	Let $\H$ be a pairwise-independent family, then
	\begin{equation}\label{eq:leftover-hash-pre-2}
		\Pr[h_S(X) = h_S[X'] \wedge X \neq X'] \leq 2^{-l} \, .
	\end{equation}
\end{exercise}
\todo{Solution}

\begin{lemma}[Technical lemma for the leftover hash]\label{lem:leftover-hash-technical}
	Let $Y$ be a random variable over a set $\Y$ such that its collision probability is at most $|\Y|^{-1} + 1 + 4\eps^2$. Then,
	\[
		\SD(Y, \unif{\Y}) \leq \eps \, .
	\]
\end{lemma}
\begin{proof}
	Define $q_y$ as
	%
	\[
		q_y = \Pr[Y = y] - \frac{1}{|\Y|}
	\]
	%
	and let $s_t$ be its sign:
	%
	\[
		s_y = \begin{cases}
			1 & \text{if } q_y \geq 0 \\
			0 & \text{otherwise.}
		\end{cases}
	\]
	%
	Then, we can define the vectors $\vec{q_y} = (q_y)_{y \in \Y}$ and $\vec{s_y} = (s_y)_{y \in \Y}$, so that
	%
	\begin{equation}\label{eq:leftover-hash-proof-1}
		\begin{aligned}[b]
			\SD(Y, \unif{\Y}) &= \frac{1}{2} \sum_{y \in \Y} \abs{\Pr[Y = y] - \frac{1}{|\Y|}} & \\
			&= \frac{1}{2} \sum_{y \in \Y} q_{y} \cdot s_{y} & \\
			&= \frac{1}{2} \DotProduct{\vec{q_y}}{\vec{s_y}} & \\
			&\leq \frac{1}{2} \sqrt{\DotProduct{\vec{q_y}}{\vec{q_y}} \cdot \DotProduct{\vec{s_y}}{\vec{s_y}}} & \text{(Law of cosines)} \\
			&= \frac{1}{2} \sqrt{\sum_{y \in \Y} q_y \cdot |\Y|}
		\end{aligned}
	\end{equation}
	%
	Let us focus only on $q_y$:
	\begin{equation}\label{eq:leftover-hash-proof-2}
		\begin{aligned}[b]
			\sum_{y \in \Y} q_y &= \sum_{y \in \Y} (\Pr[Y = y]^2 + \frac{1}{|\Y|^2} - 2 \frac{\Pr[Y = y]}{|\Y|}) & \\
			&= \Col(Y) + \frac{1}{|\Y|} - \frac{2}{|\Y|} & \\
			&= \Col(Y) - \frac{1}{|\Y|} & \\
			&= \frac{(1 + e \eps^2)}{|\Y|} - \frac{1}{|\Y|} & \text{(By assumption)} \\
			&= \frac{4\eps^2}{|\Y|}
		\end{aligned}
	\end{equation}
	
	Substituting \eqref{eq:leftover-hash-proof-2} in \eqref{eq:leftover-hash-proof-1} we have
	\begin{align*}
		\SD(Y, \unif{\Y}) &\leq \frac{1}{2} \sqrt{\frac{4\eps^2}{|\Y|} |\Y|} \\
		&= \eps
	\end{align*}
\end{proof}

\noindent Finally, we can prove lemma \ref{lemma:leftover-hash}.
\begin{proof}[Proof of leftover hash lemma.]
	Let $y = (S, h_S(x))$, $\Y = \Bool^{d+l}$ and $y' = (S', h_{S'}(X'))$. Then,
%
	\begin{flalign*}		
		\Col(Y) &= \Pr[Y = Y'] & \\
		&= \Pr[S = S' \wedge h_S(x) = h_S(x')] & \\
		&= \Pr[S = S'] \Pr[h_S(X) = h_S(X')] & \\
		&= 2^{-d} \Pr[h_S(X) = h_S(X')] & \text{(Using \ref{eq:leftover-hash-pre-1})} & \\
		&= 2^{-d} \Pr[X = X' \vee (X = X' \wedge h_S(x) = h_S(x'))] & \\
		&\leq 2^{-d} ( \Pr[h_S(x) = h_S(x') \wedge X = X'] + \Pr[X = X'] ) & \text{(Boole's ineq.)} \\
		&\leq 2^{-d}(2^{-k} + 2^{-l}) & \text{(By \ref{eq:leftover-hash-pre-1} and \ref{eq:leftover-hash-pre-2})} \\
		&= \frac{1}{2^{d+l}} (2^{l-k} + 1) & \\
		&\leq \frac{1}{2^{d+l}} (2^{2 - 2\log \eps^{-1}} + 1) & \\
		&= \frac{1}{|\Y|} (1 + 4 \eps^2)
	\end{flalign*}
%
	By lemma \ref{lem:leftover-hash-technical} we have that $\SD(Y, \unif{\Y}) \leq \eps$.
\end{proof}